---
title: "Lightning (Realtime)"
description: The Lightning ASR WebSocket API provides real-time speech-to-text transcription capabilities with streaming audio input. This API uses WebSocket to deliver transcription results as audio is processed, enabling low-latency transcription without waiting for the entire audio file to upload. Perfect for live transcription, voice assistants, and real-time communication systems that require immediate speech recognition. Supports multiple languages, speaker diarization, emotion detection, and more advanced features.
asyncapi: "/asyncapi-spec/lightning-asr-ws.json /api/v1/lightning/get_text"
---

## Query Parameters

The WebSocket connection accepts the following query parameters:

### Audio Configuration

| Parameter     | Type   | Default    | Description                                                                           |
| ------------- | ------ | ---------- | ------------------------------------------------------------------------------------- |
| `encoding`    | string | `linear16` | Audio encoding format. Options: `linear16`, `linear32`, `alaw`, `mulaw`               |
| `sample_rate` | string | `16000`    | Audio sample rate in Hz. Options: `8000`, `16000`, `22050`, `24000`, `44100`, `48000` |

### Language & Detection

| Parameter  | Type   | Default | Description                                                                                                                                                                                                                                                                                       |
| ---------- | ------ | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `language` | string | `en`    | Language code for transcription. Use `multi` for automatic language detection. Supported: `it`, `es`, `en`, `pt`, `hi`, `de`, `fr`, `uk`, `ru`, `kn`, `ml`, `pl`, `mr`, `gu`, `cs`, `sk`, `te`, `or`, `nl`, `bn`, `lv`, `et`, `ro`, `pa`, `fi`, `sv`, `bg`, `ta`, `hu`, `da`, `lt`, `mt`, `multi` |

### Feature Flags

| Parameter           | Type   | Default | Description                                                              |
| ------------------- | ------ | ------- | ------------------------------------------------------------------------ |
| `word_timestamps`   | string | `false` | Include word-level timestamps in transcription. Options: `true`, `false` |

### Webhook Configuration

## Connection Flow

### Example Connection URL

```javascript
const url = new URL("wss://waves-api.smallest.ai/api/v1/lightning/get_text");
url.searchParams.append("language", "en");
url.searchParams.append("encoding", "linear16");
url.searchParams.append("sample_rate", "16000");
url.searchParams.append("word_timestamps", "true");

const ws = new WebSocket(url.toString(), {
  headers: {
    Authorization: `Bearer ${API_KEY}`,
  },
});
```

## Input Messages

### Audio Data (Binary)

Send raw audio bytes as binary WebSocket messages:

```javascript
const audioChunk = new Uint8Array(4096);
ws.send(audioChunk);
```

### End Signal (JSON)

Signal the end of audio stream:

```json
{
  "type": "end"
}
```

## Response Format

The server responds with JSON messages containing transcription results:

```json
{
  "session_id": "sess_12345abcde",
  "transcript": "Hello, how are you?",
  "full_transcript": "Hello, how are you?",
  "is_final": true,
  "is_last": false,
  "language": "en"
}
```

### Response Fields

| Field             | Type    | Description                                                          |
| ----------------- | ------- | -------------------------------------------------------------------- |
| `session_id`      | string  | Unique identifier for the transcription session                      |
| `transcript`      | string  | Partial or complete transcription text for the current segment       |
| `full_transcript` | string  | Complete transcription text accumulated so far                       |
| `is_final`        | boolean | Indicates if this is the final transcription for the current segment |
| `is_last`         | boolean | Indicates if this is the last transcription in the session           |
| `language`        | string  | Detected language code, returns only when `is_final=True`                                               |

### Optional Response Fields (Based on Query Parameters)

| Field             | Type   | When Included            | Description                                                            |
| ----------------- | ------ | ------------------------ | ---------------------------------------------------------------------- |
| `word_timestamps` | array  | `word_timestamps=true`   | Word-level timestamps with `word`, `start`, and `end` fields           |

### Example Response with All Features

```json
{
  "session_id": "sess_12345abcde",
  "transcript": "I'm doing great, thank you!",
  "full_transcript": "Hello, how are you? I'm doing great, thank you!",
  "is_final": true,
  "is_last": true,
  "language": "en",
  "word_timestamps": [
    {
      "word": "I'm",
      "start": 1.2,
      "end": 1.4
    },
    {
      "word": "doing",
      "start": 1.4,
      "end": 1.7
    },
    {
      "word": "great",
      "start": 1.7,
      "end": 2.0
    }
  ],
```

## Code Examples

<CodeGroup>
```python python
import asyncio
import json
import argparse
import numpy as np
import websockets
import librosa
from urllib.parse import urlencode

BASE_WS_URL = "wss://waves-api.smallest.ai/api/v1/lightning/get_text"

async def stream_audio(audio_file, api_key, language="en", encoding="linear16", sample_rate=16000, word_timestamps="true"):
    params = {"language": language, "encoding": encoding, "sample_rate": sample_rate, "word_timestamps": word_timestamps}
    ws_url = f"{BASE_WS_URL}?{urlencode(params)}"
    
    async with websockets.connect(ws_url, additional_headers={"Authorization": f"Bearer {api_key}"}) as ws:
        print(f"Connected: {ws_url}")
        
        async def send():
            audio, _ = librosa.load(audio_file, sr=sample_rate, mono=True)
            chunk_size = int(0.160 * sample_rate)
            
            for i in range(0, len(audio), chunk_size):
                chunk = audio[i:i + chunk_size]
                await ws.send((chunk * 32768.0).astype(np.int16).tobytes())
                await asyncio.sleep(len(chunk) / sample_rate)
            
            await ws.send(json.dumps({"type": "end"}))
        
        sender = asyncio.create_task(send())
        
        async for message in ws:
            data = json.loads(message)
            print("Received:", {k: v for k, v in data.items() if k != 'full_transcript'})
            if data.get("is_last"):
                break
        
        await sender

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("audio_file", nargs="?", default="path/to/audio.wav")
    parser.add_argument("--api-key", "-k", default="your_api_key_here")
    parser.add_argument("--language", "-l", default="en")
    parser.add_argument("--encoding", "-e", default="linear16")
    parser.add_argument("--sample-rate", "-sr", type=int, default=16000)
    parser.add_argument("--word-timestamps", "-wt", default="true")
    
    args = parser.parse_args()
    asyncio.run(stream_audio(args.audio_file, args.api_key, args.language, args.encoding, args.sample_rate, args.word_timestamps))
```
</CodeGroup>


