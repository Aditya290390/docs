---
title: 'ASR evaluation'
description: 'Benchmark accuracy, latency, and reliability of Waves ASR'
---

# Evaluation overview

> Measure how Waves ASR performs across languages, accents, and acoustic conditions.

This playbook explains our recommended metrics, datasets, prerequisites, and example code so you can reproduce transparent benchmarks.

## What we measure

- **Word Error Rate (WER)** – primary accuracy metric, reported globally and per speaker.
- **Real-time factor (RTF)** – latency ratio of processing time vs. audio duration.
- **Cost per hour** – estimated spend using the $0.025/min rate.

## Recommended datasets

| Dataset | Domain | Notes |
| --- | --- | --- |
| Librispeech test-clean | Audiobooks | English baseline |
| Common Voice 17 | Crowdsourced | Diverse accents & microphones |
| CallHome Hindi | Conversational | Validate Hindi + English mix |
| Custom contact-center logs | Enterprise | Ensure compliance + PII masking |

Store evaluation audio inside a secured bucket and ensure transcripts are ground-truth verified.

## Prerequisites

- Python 3.10+, `torch`, `datasets`, `smallestai`, `jiwer`
- Valid enterprise API key with ASR access
- GPU optional; evaluation runs on the managed API

## Evaluation workflow

```bash
pip install smallestai datasets jiwer rich
export SMALLEST_API_KEY=YOUR_API_KEY
```

```python evaluate.py
import asyncio
from pathlib import Path

from datasets import load_dataset
from jiwer import wer
from smallestai.waves import AsyncWavesClient


async def score_batch(client, rows):
    refs, hyps = [], []
    for row in rows:
        response = await client.transcribe(
            audio_file=row["audio"]["path"],
            language=row.get("language", "en"),
            timestamp_granularities=[],
        )
        refs.append(row["transcription"])
        hyps.append(response.text)
    return wer(refs, hyps)


async def main():
    ds = load_dataset("librispeech_asr", "clean", split="test.clean[:100]")
    async with AsyncWavesClient.from_env() as client:
        error = await score_batch(client, ds)
        print(f"WER: {error:.2%}")


if __name__ == "__main__":
    asyncio.run(main())
```

**Explanation**

1. `load_dataset` fetches ground-truth audio + transcripts.
2. `AsyncWavesClient.from_env()` shares one connection for throughput.
3. `wer(refs, hyps)` computes the final metric per dataset slice.

## Output format

Emit structured JSON so you can trend results over time:

```json
{
  "dataset": "librispeech-test-clean",
  "samples": 100,
  "wer": 0.057,
  "rtf": 0.42,
  "cost_usd": 0.25,
  "timestamp": "2025-01-15T10:00:00Z"
}
```

## Benchmarks

- **English (test-clean)** – WER 5–7%, RTF < 0.5 on standard 16 kHz PCM.
- **Hindi contact-center mix** – WER 12–15% with redaction enabled.
- **Multilingual (Common Voice)** – WER varies 10–20% depending on accent coverage.

Run evaluations monthly or before major releases to ensure regression detection. Share summary dashboards with product and compliance teams for transparency.

