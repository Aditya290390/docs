---
title: 'ASR WebSocket API'
description: 'Stream audio to Waves ASR and receive real-time transcripts'
---

# ASR WebSocket API

> Real-time speech-to-text transcription using WebSocket connections.

This guide walks through the production WebSocket endpoint, authentication, streaming patterns, and a reference Python implementation. Separate platform-specific examples are available for [Node.js](/speech-to-text/developer-guides/websocket-node) and [browser microphone clients](/speech-to-text/developer-guides/websocket-browser).

## Endpoint & prerequisites

- **Production URL** – `wss://waves-api.smallest.ai/api/v1/asr`
- **Subscription** – ASR streaming is available to Enterprise Monthly or Enterprise Yearly plans.
- **SDKs & tooling** – Python 3.9+, `websockets` 11+, Node 18+, and HTTPS/WSS egress from your environment.
- **Audio encodings** – linear16 (recommended), FLAC, μ-law, or Opus with 8–48 kHz sample rates.

## Authentication

Pass your API key in the `Authorization` header when initiating the WebSocket handshake:

```http
Authorization: Bearer YOUR_API_KEY
```

If you are connecting from a browser, proxy the request through your backend to avoid exposing secrets.

## Connection workflow

<Steps>
  <Step title="Obtain API key">
    Create or rotate keys in the console, then store them securely in your backend or secret manager.
  </Step>
  <Step title="Negotiate WebSocket">
    Connect to `wss://waves-api.smallest.ai/api/v1/asr` with the `Authorization` header and optional query parameters (`language`, `sample_rate`, `format`, etc.).
  </Step>
  <Step title="Stream audio frames">
    Read PCM frames from a file, microphone, or media stream, chunk to 20–50 ms buffers, and send them as binary messages.
  </Step>
  <Step title="Consume responses">
    Handle JSON messages that include interim hypotheses, `final` results, and `end_of_turn` markers.
  </Step>
</Steps>

## Python streaming reference

```python websocket_client.py
import asyncio
import json
import os
import wave

import websockets

ASR_URL = "wss://waves-api.smallest.ai/api/v1/asr"
API_KEY = os.environ["SMALLEST_API_KEY"]


async def stream_file(path: str, sample_rate: int = 16000):
    headers = {"Authorization": f"Bearer {API_KEY}"}
    params = f"?language=en&sample_rate={sample_rate}&format=linear16"

    async with websockets.connect(f"{ASR_URL}{params}", extra_headers=headers) as ws:
        with wave.open(path, "rb") as source:
            while True:
                chunk = source.readframes(320)  # ~20 ms @16 kHz
                if not chunk:
                    break
                await ws.send(chunk)

                response = json.loads(await ws.recv())
                if "text" in response:
                    print("Partial:", response["text"])
                if response.get("type") == "final":
                    print("Final:", response["text"])

        await ws.send(json.dumps({"type": "end_of_turn"}))


if __name__ == "__main__":
    asyncio.run(stream_file("sample.wav"))
```

## Configurable parameters

| Parameter | Description | Example |
| --- | --- | --- |
| `language` | ISO language code | `language=en` |
| `sample_rate` | Audio sample rate in Hz | `sample_rate=16000` |
| `format` | Audio encoding (`linear16`, `flac`, `mulaw`, `opus`) | `format=opus` |
| `punctuation` | Toggle smart punctuation | `punctuation=true` |
| `redact` | Enable PCI/PII redaction | `redact=ssn,card` |
| `vad` | Emit VAD events | `vad=true` |

## Feature highlights

- Multi-language streaming with configurable punctuation and redaction.
- Voice activity detection events to align transcripts with speaker turns.
- Sensitive data controls to mask PCI, SSN, and numeric spans inline.

For optimization guidance and troubleshooting steps, see:

- [Input & preprocessing best practices](/speech-to-text/developer-guides/audio-prep)
- [Lightning ASR best practices](/speech-to-text/developer-guides/best-practices)
- [Lightning ASR troubleshooting](/speech-to-text/developer-guides/troubleshooting)

